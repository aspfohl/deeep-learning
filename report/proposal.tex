\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022

% ready for submission
\usepackage[preprint, nonatbib]{neurips_2022}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{amsmath, amssymb, amscd, amsthm, amsfonts}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pythonhighlight}


\title{3D Stylized Avatars from a Single Image}

\author{%
  Mahir Kothary\\
  Computer Science\\
  Cornell Tech\\
  NYC, NY 10044 \\
  \texttt{mk942@cornell.edu} 
 \And Anna Pfohl\\
  Computer Science\\
  Cornell Tech\\
  NYC, NY 10044 \\
  \texttt{als462@cornell.edu}
\AND
  Gustavo Vasquez \\
  Computer Science\\
  Cornell Tech\\
  NYC, NY 10044 \\
  \texttt{av384@cornell.edu}
\And
  Jia Zhao \\
  Computer Science\\
  Cornell Tech\\
  NYC, NY 10044 \\
  \texttt{jz538@cornell.edu}
}

\begin{document}

\maketitle

\begin{abstract}
In this paper, we propose a novel method for generating 3D avatars using a single clothing reference image and a style reference image. Our method combines previous work that generates Monocular-to-3D projections and style transfer models.
\end{abstract}


\section{Introduction}
% What is the problem you are working on and why is it important? 


% We want to apply transformer-based style transfer to fashion design and compare the results with previous research using CNN-based style transfer. Fashion design is the art of applying design, style and texture synthesis to create billions of dresses, suits, shoes and bags in our daily life. Artworks generated using state-of-the-art image techniques are already creative and impressive to human eyes, but fashion design using the same techniques is still far from attractive due to the limits of CNN-based style transfer. Novel research combining transformers and style transfer has become a trending topic in the past few months. The release of Nvidia's transformer-based GPU H100 \footnote{\href{https://www.nvidia.com/en-us/data-center/h100/}{Nvidia GPU H100}} also demonstrates the demand and potential of transformers in deep neural networks. 


\subsection{Related Work}
% Whatâ€™s been done on the problem so far? What methods have been tried? Can you list papers that have done similar things and talk about that? Definitely make it clear that you have looked at papers doing similar things more words in the paper compared to earlier years.

\textbf{Style Transfer} Style transfer learns the style of one image, usually artworks from distinctive artists like Van Gogh and Monet, and applies it to a target image while preserving the content of the target image. The problem of style transfer is closely related to texture synthesis and rendering, which requires an explicit separation of image content from style. Early approaches [citations] often failed on the separation due to the lack of image representations for semantic information. Style transfer became achievable after Gatys et al. proposed the seminal algorithm mapping the semantic features to the convolutional layers of CNNs and demonstrated that image content and image style can be separated and recombined \cite{gatys2016styleTransfer}. 

TODO: style transfer models

% [maybe include disadvantages?]

% \textbf{Generative Adversarial Networks} GANs consist of a discriminator and a generator that competitively process backpropagation signals to learn deep representations with only a relatively small amount of training data \cite{goodfellow2014GAN, creswell2018GANoverview}. The adversarial loss of the discriminator will force the generator to generate images indistinguishable from real images. This characteristic of GANs has advanced image generation \cite{denton2015deepGAN, radford2015unsupervisedGAN}. Empirically, GANs have achieved the best visual quality of generated images. Various models have been proposed to combine GANs with style transfer, including \cite{Gated-GAN, DRB-GAN, AgarPlateGAN}. Artistic style transformers have been widely accessible online to transfer a daily photo to Van Gogh's style like the Starry Night. 

% \textbf{Fashion Design Using GANs} Fashion design seems like an unexplored world for style transfer. Current research mainly focuses on virtual try-on with white background and the generated clothes are usually in simple patterns \cite{kato2019ganClothing, yildirim2019customOutfits, rostamzadeh2018fashion-gen, hsiao2019fashion++, ak2019attributeGAN, liu2021fashionVisTool}. More interesting research such as \cite{wang2020fashionSegmentation} leverages image segmentation to identify the clothing in a more complicated background and transfers the style to artistic work with relatively high resolution. 

% \textbf{Transformers} Transformers were initially designed for NLP tasks, but has started to gain better accuracy in vision tasks including segmentation, image classification and visual reasoning compared to the state-of-the-art CNN architectures \cite{khan2021CVtransSurvey}. Novel research in past months has demonstrated that, overall, transformers perform much more efficiently than CNNs with the same data sets and finetuning procedures \cite{malpure2021finetuneTrans}. StyTr2 proposed by \cite{deng2021stytr} is the first transfermor-based style transfer with encoders for both image content and image style. Transformer-based texture synthesis in style transfer has shown better result in \cite{lu2022transTextureSynthesis} compared to CNN-based models. The transformer-inspired StyleFormer proposed in \cite{wu2021styleformer} is a visually real-time style transfer method and generates coherent textures with fine details. 

\textbf{3D related literatures}



\section{Proposed Work}

Note: fit dress to ppl wearing tops and pants, fit tops (and auto-generate the pants) to ppl wearing dresses. 

% What are you going to do? Are you going to compare methods? Are you going to change methods? What methods  will you be comparing to? What metrics will you use to know if you succeed?

% You should list the questions the project will address and that will be discussed in the report.

% \subsection{Software}
% You should list what software you will be using or will build upon.

\subsection{Pre-trained models}
% What datasets are you going to use?

% We will consider using pre-trained fashion models including \cite{todo}, as well as many open-source datasets for training and fine-tuning. For images, we will consider Kaggle's fashion product dataset (44,400 images) \footnote{\href{https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-dataset}{Kaggle's Fashion Product Images Dataset}} and Fashion-Gen (293,008 images) \cite{rost2018fashiongen}, which both include labels and various fashion items (e.g. shirts, dresses, pants, etc) from various angles. Since textures may be more difficult to generate we could also consider integrating the describable textures dataset \cite{cimpoi14describing}. Lastly, many 3D human pose datasets exist such as Human3.6M \cite{Ionescu2011pose}\cite{Ionescu2014pose}, which could serve as a starting point for projecting clothing into three dimensional space.

% Describe the datasets you will use and how will you know if the project is successful.
% Describe the hypotheses you will test and the related work.


% https://arxiv.org/abs/2108.05126
% what style transfer model

\subsection{Evaluation}
% What results did others get on those datasets?
Evaluating the performance of our proposed work can be challenging because results of generative models can often be subjective \cite{borji2018eval}. We plan to use a combination of human judgement and quantitative metrics to benchmark our results to other benchmarks for this application \cite{todo}.

\textbf{Human Judgement} Many image generation models have depended on surveys and/or Amazon Mechanical Turk \cite{zhou2019hype} to do image evaluation, focusing on ranking results against each other, rating qualities of results across different dimensions, or even serving as a Turing test (e.g. Is this image real or AI-generated) \cite{salimans2016eval}. We plan to send out randomized surveys to the Cornell Tech community that will assess the output of our model in this way.

% \textbf{Inception Score} The inception score is often used for GANs to measure two things at the same time: (1) variety or entropy of the output and (2) similarity over known data (i.e. image actually looks like something in the training set) \cite{salimans2016eval}. Existing literature found that inception score strongly correlates with human judgment \cite{szegedy2015is}. Limitations of this metric include that the score is highly dependent on the training set (i.e. unknown classes will have low inception scores) and increased memorization of the training set. For this reason, we plan to leverage both human and quantitative evaluation methods in tandem.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Date} & \textbf{Milestone Completed}                \\ \hline
April 1       & Background research and literature review     \\ \hline
April 8       & Pre-trained models collected and prepared \\ \hline
April 15      & Model pipeline prototype complete
\\ \hline
April 21      & Preliminary results, judged by team         \\ \hline
May 1         & Model pipeline finalized, surveys sent out  \\ \hline
May 10        & Final results visualized and report created \\ \hline
\end{tabular}
\caption{Expected project timeline}
\end{table}






\newpage
\bibliographystyle{acm}
\bibliography{ref}



% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.5\textwidth]{}
%     \caption{Caption}
%     \label{fig:my_label}
% \end{figure}



\end{document}
