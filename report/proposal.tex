\documentclass{article}

% ready for submission
\usepackage[preprint, nonatbib]{neurips_2022}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{amsmath, amssymb, amscd, amsthm, amsfonts}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pythonhighlight}

\title{3D-SARI: 3D Stylized Avatars from a Rendered Image}

\author{%
  Mahir Kothary\\
  Computer Science\\
  Cornell Tech\\
  NYC, NY 10044 \\
  \texttt{mk942@cornell.edu} 
 \And Anna Pfohl\\
  Computer Science\\
  Cornell Tech\\
  NYC, NY 10044 \\
  \texttt{als462@cornell.edu}
\AND
  Gustavo Vasquez \\
  Computer Science\\
  Cornell Tech\\
  NYC, NY 10044 \\
  \texttt{av384@cornell.edu}
\And
  Jia Zhao \\
  Computer Science\\
  Cornell Tech\\
  NYC, NY 10044 \\
  \texttt{jz538@cornell.edu}
}

\begin{document}

\maketitle


\begin{abstract}
In this paper we present 3D Stylized Avatars from a Rendered Image (3D-SARI), a mechanism to generate a stylized 3D avatar from a single clothing reference and a style reference image. Existing methods in fashion have used 3D models to create virtual try-on projections or have used style transfer to generate new 2D fashion items from a style reference image and a segmented outfit. Our novel method combines both Monocular-to-3D projections for virtual try-on projections and style transfer for fashion design to generate 3D clothed avatars.
\end{abstract}

\section{Introduction}
% What is the problem you are working on and why is it important? 
Fashion design is the art of applying design, style and textiles to create clothing articles such as dresses, suits, shoes, and bags. The intensive requirements of training and creativity in fashion design limits the general community from designing their own clothing. The process of designing a 2D clothing and fitting it onto a 3D human model has been empowered by computer vision techniques such as style transfer and 3D modeling. Increasing attention has raised in this process for its commercial value in real world fashion design and its potential in virtual avatar design in gaming and the metaverse.

Several prior works have explored 2D fashion design using segmentation and different style transfer models and GANs. MMFashion\cite{liu2021fashionVisTool} and fashion segmentation\cite{wang2020fashionSegmentation} understand and visualize fashion items and further segment the fashion items from a person wearing them. More advanced technologies have built on these systems to use the segmentation to help generate clothing try-on mechanisms\cite{Pang2021tryon, Tiwari2021tryon}. Artwork generated using state-of-the-art image techniques are already impressive to human eyes, but fashion design using the same techniques poses an interesting challenge due to limits of fine-grained details in 2D images. Novel research leveraging 3D modeling has provided a new approach to recover and reconstruct fine-grained clothing details in 3D virtual try-on projections. The aim of this paper is to explore fashion design on 3D models using  paint 2D fashion clothing generated by style transfer onto 3D reconstructions of human models.  


\subsection{Related Work}
% Whatâ€™s been done on the problem so far? What methods have been tried? Can you list papers that have done similar things and talk about that? 

\textbf{Style Transfer} Style transfer learns the style of one image (e.g. art piece by Jackson Pollock), and applies it to a target image while preserving the content of the target image. The problem of style transfer is closely related to texture synthesis and rendering, which requires an explicit separation of image content from style. Early approaches\cite{Hertzmann2001st, Ashikhmin2003st} often failed on the separation due to the lack of image representations for semantic information. Style transfer became achievable after Gatys et al. proposed the seminal algorithm mapping the semantic features to convolutional layers and demonstrated that image content and image style can be separated and recombined \cite{gatys2016styleTransfer}. 

\textbf{Style Transfer Using GANs} GANs consist of a discriminator and a generator that competitively process backpropagation signals to learn deep representations with only a relatively small amount of training data \cite{creswell2018GANoverview, goodfellow2014GAN}. The adversarial loss of the discriminator will force the generator to generate images indistinguishable from real images. This characteristic of GANs has advanced image generation \cite{denton2015deepGAN, radford2015unsupervisedGAN}. Fashion design seems like an unexplored world for style transfer. Current research mainly focuses on virtual try-on with white background and the generated clothes are usually in simple patterns \cite{kato2019ganClothing, liu2021fashionVisTool, yildirim2019customOutfits}. More interesting research such as \cite{wang2020fashionSegmentation} leverages image segmentation to identify the clothing in a more complicated background and transfers the style to artistic work with relatively high resolution. However, GANs are computation intensive and the results of GAN-based fashion design are not satisfying to human eyes due to the limits of fine-grained details in 2D images.

\textbf{3D Modeling} High-fidelity 3D reconstruction with fine details of humans is the key to 3D fashion design. Previous approaches usually require expensive professional camera systems to capture the objects from multi views. Facebook researchers proposed an alternative with an end-to-end trainable framework in PIFuHD \cite{saito2020PIFuHD} to reconstruct clothed humans at high resolution from a single image without post-processing. In a more recent work \cite{zhureef}, Zhu et al. proposed ReEF to reconstruct clothing with surface details such as drape and winkles from clothes humans in single images. With fine-grained surface details, model M3D-VTON \cite{m3dvton} converts 2D clothing images to 3D try-on with multi views. The transformer-inspired StyleFormer proposed in \cite{wu2021styleformer} is a visually real-time style transfer method and generates coherent textures with fine details. 


% \textbf{Transformers} Transformers were initially designed for NLP tasks, but has started to gain better accuracy in vision tasks including segmentation, image classification and visual reasoning compared to the state-of-the-art CNN architectures \cite{khan2021CVtransSurvey}. Novel research in past months has demonstrated that, overall, transformers perform much more efficiently than CNNs with the same data sets and finetuning procedures \cite{malpure2021finetuneTrans}. StyTr2 proposed by \cite{deng2021stytr} is the first transfermor-based style transfer with encoders for both image content and image style. Transformer-based texture synthesis in style transfer has shown better result in \cite{lu2022transTextureSynthesis} compared to CNN-based models. 


\section{Proposed Work}
% What are you going to do? Are you going to compare methods? Are you going to change methods? What methods  will you be comparing to? What metrics will you use to know if you succeed?

% You should list the questions the project will address and that will be discussed in the report.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{report/diagrams/3d_sari.png}
    \caption{Proposed system architecture: Our architecture is broken into two components, the style transfer and the monocular to 3D representation. Above, we use VGG-19 to extract features from the style reference image, and image segmentation algorithms to segment the outfit from the user. We then use M3D-VTON to generate the 3D stylized avatars from the newly created stylized outfit.}
\end{figure}

\subsection{Models}
\textbf{Style Transfer} Prior models from Gatys et al.\cite{gatys2016styleTransfer} have successfully used style transfer using art and applied them to other reference images. These have primarily been done using pre-trained VGG-models to extract key features from the style image, and applying it to the subject image. We aim to continue building on the same model, using the VGG-19 model to extract key features from our style images and applying them to generate stylized fashion pieces. 

\textbf{Human Modelling and Rendering} Zhao et al. released M3D-VTON\cite{m3dvton} with the purpose of providing a comprehensive three-pronged model to allow a 2D monocular image of a person to be converted into a 3D image, while taking into account the texture and fitting of the fashion piece. We will use the pre-trained models for Monocular Recognition, which is required to generate a clothes agnostic version of the person, and Depth Refinement to capture the angle and pose of the person. Lastly, we will combine M3D-VTON's Texture Fusion model with the style transfer model to generate clothing which is warped to fit the generated avatar. 

\subsection{Evaluation Benchmarks}
% What results did others get on those datasets? - 
While to our knowledge there are no examples of previous work that have combined style transfer and 3D rendering in fashion, we plan to benchmark our results with other known style transfer fashion applications for 2D images\cite{date2017fashion2d}\cite{wang2020fashionSegmentation}.

\subsection{Evaluation Metrics}
% Describe the hypotheses you will test and the related work. how will you know if the project is successful.
To evaluate our model's success, we plan to use a combination of the Structural SIMilarity index measure (SSIM)\cite{wang2015eval}, the Frechet Inception Distance (FID)\cite{salimans2016eval}, and human evaluation (HE) similar to the M3D-VTON evaluation methods\cite{m3dvton}. As discussed by Gatys\cite{gatys2016styleTransfer} and others\cite{borji2018eval}\cite{zhou2019hype}, evaluating the success of style transfer models can be highly subjective so we plan to leverage HE for evaluation and comparison against the model benchmark listed above. We plan to send out randomized surveys to the Cornell Tech community that will assess the output of our model by ranking models against each other and output across different dimensions (e.g. realism, quality of texture, similarity to style reference etc). Our hypothesis is that are model combination methods will be more successful for stylized 3D renderings than the benchmarks alone, and results will be comparable to the 2D rendering models but in three dimensions.

\subsection{Dataset}
% Describe the datasets you will use 
Because we will rely heavily on assembling pre-trained models, we will avoid needing to do large amounts of training. The models we will use will have already been trained on the MPV-3D dataset\cite{dong2019data1}. We will consider utilizing these if fine-tuning the entire architecture is needed. Additionally, we expect to use Human3.6M\cite{Ionescu2014pose}, Fashion-Gen\cite{rost2018fashiongen}, and Kaggle \href{https://www.kaggle.com/datasets/ikarus777/best-artworks-of-all-time}{Best Artworks of All Time} datasets for generating evaluation output and examples.

\subsection{Infrastructure}
% You should list what software/infrastructure you will be using or will build upon.
% Can we run it?
Our approach consists of two basic steps: 3D Model Generation and Style Rendering. For the model generation step, we plan to leverage pre-trained models \cite{m3dvton}. We expect this step to require minimal resources and training time (even if extra training for fine-tuning is needed). For the style rendering step, we first must focus on style transfer techniques. In this project, we plan to use the style transfer approach used by Gatys et al. \cite{gatys2016styleTransfer}, which requires feature extraction using VGG models. The next step, perhaps the most challenging, is to apply 3d rendering, for which we must learn to use specialized libraries and tools like StyleMesh, PyTorch3d and MeshLab. 

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Date} & \textbf{Milestone Completed}                \\ \hline
April 1       & Background research and literature review     \\ \hline
April 8       & Models \& data collected \& assessed \\ \hline
April 15      & Model pipeline prototype complete
\\ \hline
April 21      & Preliminary results, judged by team         \\ \hline
May 1         & Model pipeline finalized, surveys sent out  \\ \hline
May 10        & Final results visualized and report created \\ \hline
\end{tabular}
\caption{Expected project timeline}
\end{table}

\newpage
\bibliographystyle{acm}
\bibliography{ref}
\end{document}
